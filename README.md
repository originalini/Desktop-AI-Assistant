# Локальный чат с нейросетью на C++

Это простое десктопное приложение для локального чата с нейросетью. Все вычисления происходят на вашем компьютере, ничего не отправляется в интернет.

Проект разработан на C++ с использованием wxWidgets для интерфейса и llama.cpp для работы с нейросетью.

### Что нужно для сборки: 

* Git 
* Cmake (версия 3.15 или выше)
* Компилятор с поддержкой С++11 и выше


Библиотеки wxWidgets и llama.cpp уже включены в проект в папку library/, поэтому их не нужно устанавливать отдельно.

Как собрать и запустить

### 1. Клонируйте репозиторий:

```bash
git clone https://github.com/originalini/Desktop-AI-Assistant.git
cd "Your file"
```

### 2. Создайте папку для сборки и запустите CMake:

```bash
mkdir build
cd build
cmake ..
```

### 3. Скомпилируйте проект:

```bash
cmake --build . --config Release
```


### 4. Запустите приложение:

Скачайте любую модель в формате .gguf (например, с [Hugging Face](https://huggingface.co/)).

Отредактируйте путь к модели: 

Откройте файл source/UI_manager.cpp и в строке statusLoadModel = model->InitAI(...) укажите правильный путь к вашему файлу модели.

Исполняемый файл My_AI-Assistent.exe будет находиться в папке build/Release. Запустите его.


> [!IMPORTANT]
> Текущие инструкции и конфигурация сборки (CMakeLists.txt) предназначены в первую очередь для **Windows**.


> [!IMPORTANT] 
> Примечание про GPU:Текущая конфигурация собирает проект для работы на CPU. Чтобы использовать ускорение на видеокарте (NVIDIA/AMD), необходимо пересобрать библиотеку llama.cpp с соответствующими флагами (например, -DLLAMA_CUDA=ON). 
>
> Подробные инструкции можно найти в официальном репозитории [llama.cpp](https://github.com/ggml-org/llama.cpp).
